# Phase 3, Step 2: Slurm & Apptainer Wrappers

* **File:** `3.2-slurm-wrappers.md`
* **Date:** December 18, 2025
* **Target:** Node 01 (Head Node)
* **Goal:** Create user-friendly wrapper scripts to abstract the container and scheduler complexity.

---

## 1. Overview

To "democratize access" as per the Project Plan, users should not need to memorize complex Apptainer commands or manually write SBATCH headers for standard jobs.

We will deploy two types of wrappers to `/usr/local/bin`:

1. **Interactive Wrappers:** Commands like `root` or `mvme2xy` that instantly launch the tool inside the container, behaving like native applications. These run on the **Head Node**.
2. **Submission Wrappers:** Commands like `sub_talys` that automatically generate a Slurm job script and submit it to the cluster. These run the actual physics on the **Compute Nodes**.

**Container Location:** `/home/software/containers/lab-physics-v1.sif`

## 2. Interactive Wrappers

These scripts allow users to run utilities directly. We use `"$@"` to pass all arguments from the user to the containerized tool.

### 2.1 Backup Legacy Binaries (Important)

Since we are replacing the "native" commands with container wrappers, we must move the original binaries aside to prevent confusion.

    # Rename original binaries to .legacy
    sudo mv /usr/local/bin/mvme2xy /usr/local/bin/mvme2xy.legacy
    sudo mv /usr/local/bin/dat2xy /usr/local/bin/dat2xy.legacy
    
    # (Optional) Verify they are moved
    ls -l /usr/local/bin/*.legacy

### 2.2 The 'root' Wrapper

This wrapper launches CERN ROOT immediately on the current machine (Head Node).

    sudo nano /usr/local/bin/root

Content:

    #!/bin/bash
    # Wrapper to launch CERN ROOT from the Lab Container
    apptainer exec /home/software/containers/lab-physics-v1.sif root "$@"

### 2.3 The 'mvme2xy' Wrapper

This wrapper replaces the legacy binary. It runs the containerized version of `mvme2xy`.

    sudo nano /usr/local/bin/mvme2xy

Content:

    #!/bin/bash
    # Wrapper for MVME conversion tool
    apptainer exec /home/software/containers/lab-physics-v1.sif mvme2xy "$@"

### 2.4 The 'dat2xy' Wrapper

    sudo nano /usr/local/bin/dat2xy

Content:

    #!/bin/bash
    # Wrapper for legacy DAT conversion
    apptainer exec /home/software/containers/lab-physics-v1.sif dat2xy "$@"

### 2.5 Apply Permissions

    sudo chmod +x /usr/local/bin/root /usr/local/bin/mvme2xy /usr/local/bin/dat2xy

## 3. Submission Wrappers (The "Easy Button")

These scripts replace the need to manually write `.sbatch` files. They wrap the user's input in Slurm logic and **submit it to the compute nodes**.

### 3.1 The 'sub_talys' Wrapper

Talys reads from standard input. This script creates a job that pipes the user's input file into the containerized Talys executable.

    sudo nano /usr/local/bin/sub_talys

Content:

    #!/bin/bash
    
    # Usage Check
    if [ -z "$1" ]; then
        echo "Usage: sub_talys <input_file> [job_name]"
        exit 1
    fi
    
    INPUT_FILE=$1
    JOB_NAME=${2:-talys_job}
    
    # Generate a temporary submission script
    cat <<EOF > submit_temp.sh
    #!/bin/bash
    #SBATCH --job-name=$JOB_NAME
    #SBATCH --output=%x.out
    #SBATCH --error=%x.err
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=1
    #SBATCH --time=04:00:00
    #SBATCH --partition=normal
    
    echo "Running Talys on \$(hostname)"
    
    # Run Talys inside the container, piping the input file
    apptainer exec /home/software/containers/lab-physics-v1.sif talys < $INPUT_FILE
    EOF
    
    # Submit the job
    sbatch submit_temp.sh
    rm submit_temp.sh
    echo "Job submitted for $INPUT_FILE"

### 3.2 The 'sub_geant4' Wrapper

Geant4 users usually compile their own executable (e.g., `exampleB1`). This wrapper takes that executable and runs it on the cluster.

    sudo nano /usr/local/bin/sub_geant4

Content:

    #!/bin/bash
    
    if [ -z "$1" ] || [ -z "$2" ]; then
        echo "Usage: sub_geant4 <executable> <macro_file>"
        echo "Example: sub_geant4 ./exampleB1 run1.mac"
        exit 1
    fi
    
    EXE=$1
    MACRO=$2
    JOB_NAME=$(basename $EXE)
    
    cat <<EOF > submit_geant4.sh
    #!/bin/bash
    #SBATCH --job-name=$JOB_NAME
    #SBATCH --output=%x.out
    #SBATCH --error=%x.err
    #SBATCH --nodes=1
    #SBATCH --ntasks=1
    #SBATCH --cpus-per-task=20
    #SBATCH --time=24:00:00
    #SBATCH --partition=normal
    
    echo "Running Geant4 Simulation on \$(hostname)"
    
    # Execute the user's binary inside the container environment
    apptainer exec /home/software/containers/lab-physics-v1.sif $EXE $MACRO
    EOF
    
    sbatch submit_geant4.sh
    rm submit_geant4.sh
    echo "Simulation submitted: $EXE with $MACRO"

### 3.3 Apply Permissions

    sudo chmod +x /usr/local/bin/sub_talys /usr/local/bin/sub_geant4

## 4. Helper: The 'lab-build' Wrapper

Since users cannot compile Geant4 code on the Head Node directly (it lacks the libraries), they must compile *through* the container.

    sudo nano /usr/local/bin/lab-build

Content:

    #!/bin/bash
    # Helper to run cmake/make inside the container context
    echo "Entering Lab Build Environment..."
    apptainer exec /home/software/containers/lab-physics-v1.sif bash -c "cmake . && make -j\$(nproc)"

*Usage:* A student goes into their source folder and types `lab-build`. The container spins up on the Head Node, compiles their code, and exits.

## 5. Verification

**Run these tests from the Head Node.**

1. **Test Interactive Wrapper:**

        root -b -q
        # Expected: ROOT starts and exits without error.
        # This confirms the container works locally.

2. **Test Submission Wrapper:**
    Create a dummy input file `test.inp`:

        Au
        197

    Submit it:

        sub_talys test.inp

    Check the queue:

        squeue
        # Expected: You see a job labeled 'talys_job' running on node02, node03, or node04.

## 6. Next Steps

With the wrappers in place, the interface is ready. Now we must validate the performance.

Proceed to **3.3-final-validation.md**.
