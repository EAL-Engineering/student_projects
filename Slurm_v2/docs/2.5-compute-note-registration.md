# Phase 2, Step 4: Compute Node Registration

* **File:** `2.4-node-registration.md`
* **Date:** December 10, 2025
* **Target:** Node 01 (Head Node)
* **Software:** Warewulf 4.x, ipmitool, iperf3, Slurm
* **Goal:** Configure, register, and boot all compute nodes, then validate network throughput and scheduler connectivity.

---

## 1. Overview

With the VNFS image built (Step 2.2) and the Network Overlays configured (Step 2.3), we must now register the actual hardware.

We will add three compute nodes: `node02`, `node03`, and `node04`. These nodes will be configured to:

1. Boot via PXE (using the MAC address of their primary interface).
2. Receive a static IP assignment (mapped to the bond interface created in the overlay).
3. Inherit the `default` profile settings (Kernel + Container + Overlays).

**Prerequisites:** You must have the MAC address for the primary 10GbE interface (`eno1`) of each Supermicro node.

## 2. Node Naming & Addressing Scheme

Consistent naming is vital for Slurm and host resolution. We will follow the schema defined in the Project Plan, using the MAC addresses identified in the hardware inventory.

**Physical Orientation (Rear View):**
Based on the visual labels in the rack diagram (`backofbrik.jpg`), the nodes are arranged as follows:

    -------------------------
    |  Node D   |  Node B   |
    | (Node 04) | (Node 02) |
    -------------------------
    |  Node C   |  Node A   |
    | (Node 03) | (Node 01) |
    -------------------------

**Registration Table:**

| Node Name | Role | IP Address | Hardware (MAC) Address | Source Node |
| :--- | :--- | :--- | :--- | :--- |
| **node02** | Compute | 10.0.0.92 | `AC:1F:6B:C4:C4:66` | Node B (Top Right) |
| **node03** | Compute | 10.0.0.93 | `AC:1F:6B:59:A0:B6` | Node C (Bottom Left) |
| **node04** | Compute | 10.0.0.94 | `AC:1F:6B:5A:BD:04` | Node D (Top Left) |

## 3. Registering Nodes

Use `wwctl` to add the nodes to the database. We explicitly set the IP address here so that the Warewulf template engine can inject it into the `bond0.nmconnection.ww` overlay we created in Step 2.3.

Register Node 02:

    sudo wwctl node add node02 --ipaddr 10.0.0.92 --hwaddr AC:1F:6B:C4:C4:66

Register Node 03:

    sudo wwctl node add node03 --ipaddr 10.0.0.93 --hwaddr AC:1F:6B:59:A0:B6

Register Node 04:

    sudo wwctl node add node04 --ipaddr 10.0.0.94 --hwaddr AC:1F:6B:5A:BD:04

## 4. Configuring IPMI

Since these are Supermicro X10 "Twin" nodes, they have dedicated IPMI (BMC) interfaces. Configuring this in Warewulf allows you to issue power commands (e.g., `wwctl power cycle node02`) directly from the Head Node.

**Assumption:** IPMI interfaces are on the `10.0.0.1xx` range.

Configure IPMI settings for all nodes:

    sudo wwctl node set node02 --ipmiaddr 10.0.0.102 --ipmiuser ADMIN --ipmipass ADMIN
    sudo wwctl node set node03 --ipmiaddr 10.0.0.103 --ipmiuser ADMIN --ipmipass ADMIN
    sudo wwctl node set node04 --ipmiaddr 10.0.0.104 --ipmiuser ADMIN --ipmipass ADMIN

## 5. Verify Configuration

Before attempting a boot, verify that the nodes are correctly associated with the `default` profile (which contains the `almalinux-9-hpc` container and `lacp-bond` overlay).

List all nodes and their profiles:

    sudo wwctl node list -a

**Expected Output:**
Ensure that the `Profiles` column lists `default` for all three nodes.

## 6. Persist Changes

Warewulf requires a configuration reload to update the underlying TFTP and HTTP provisioners with the new node data.

Reload Warewulf configuration:

    sudo wwctl configure --all

## 7. Booting the Cluster

We will boot the cluster in stages to ensure the first node comes up correctly before flooding the network with PXE requests.

### 7.1 Boot Node 02 (Pilot)

Manually power on Node 02 (Top Right) or use IPMI:

    ipmitool -I lanplus -H 10.0.0.102 -U ADMIN -P ADMIN power on

**Monitor:** Use IPMI SOL (`sol activate`) or a VGA monitor to watch the boot process. You should see:

1. PXE DHCP Request (successful).
2. iPXE fetching the kernel/initramfs.
3. Linux boot log.
4. Login prompt.

### 7.2 Boot Remaining Nodes

Once Node 02 is successfully at a login prompt, repeat the process for the remaining nodes.

    ipmitool -I lanplus -H 10.0.0.103 -U ADMIN -P ADMIN power on
    ipmitool -I lanplus -H 10.0.0.104 -U ADMIN -P ADMIN power on

## 8. Validation Tests

Now that the nodes are online, we will verify the infrastructure goals defined in the Project Plan.

### 8.1 Network Throughput (LACP Verification)

We need to confirm that the LACP bond is active and providing improved bandwidth.

**On the Head Node (Server):**
Ensure `iperf3` is installed (`dnf install iperf3`) and start it in server mode:

    iperf3 -s

**On Node 02 (Client):**
SSH into the node and run the test against the head node:

    ssh node02
    iperf3 -c 10.0.0.91 -P 4

**Success Criteria:**

* A single stream might only show ~9.4 Gbps (limit of a single flow).
* The aggregated `-P 4` (parallel streams) result should approach **18â€“19 Gbps**.

### 8.2 Scheduler Connectivity (Slurm)

We installed the Slurm client and daemon in the VNFS image (Step 2.2). We can now verify if the nodes have registered with the controller.

Run the Slurm info command on the Head Node:

    sinfo

**Success Criteria:**

* **PARTITION:** Should list your default partition (e.g., `normal` or `debug`).
* **NODES:** Should list `node[02-04]`.
* **STATE:** Should be `idle` (Green/Good).
  * *Note:* If state is `down` or `drain`, check `/var/log/slurm/slurmctld.log` for "Invalid Credential" errors (indicating a Munge key mismatch).

## 9. Troubleshooting

* **"Node not found" during PXE:** Double-check the MAC address entered in Step 3.
* **iperf3 shows < 10Gbps:** Verify the bond status on the compute node:
    `cat /proc/net/bonding/bond0`
    Ensure it says "Bonding Mode: IEEE 802.3ad Dynamic link aggregation".
* **Slurm nodes are "down":** Restart the slurm daemon on the compute node to force a re-registration:
    `ssh node02 "systemctl restart slurmd"`
