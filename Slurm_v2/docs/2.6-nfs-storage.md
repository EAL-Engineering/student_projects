# Phase 2, Step 6: Configuring NFS Shared Storage

* **File:** `2.6-nfs-storage.md`
* **Date:** December 12, 2025
* **Target:** Node 01 (Head Node)
* **Software:** NFSv4, Warewulf Overlays
* **Goal:** Configure the Head Node to export `/home` and `/home/software` to the compute cluster, and configure compute nodes to auto-mount these shares.

---

## 1. Overview

To support a unified user environment, the cluster requires shared storage.

1. **User Home Directories:** Users must see the same files (source code, data, scripts) regardless of which node they log into.
2. **Software Repository:** The Apptainer container images (created in Phase 3) must be accessible to all nodes over the network.

We will use the Head Node (Node 01) as the NFS Server and configure the compute nodes as NFS Clients via a Warewulf overlay.

## 2. Server-Side Configuration (Head Node)

First, we configure Node 01 to export the `/home` directory to the internal network.

### 2.1 Install and Enable NFS

Ensure the NFS utilities are installed and the service is running.

    dnf install nfs-utils
    systemctl enable --now nfs-server

### 2.2 Configure Exports (Restricted)

We will restrict access to the `10.0.0.64/27` subnet. This covers the new cluster nodes (91-94) as well as the legacy DAQ and Slurm machines, while blocking the rest of the campus network.

Edit `/etc/exports`:

    nano /etc/exports

Add the following line:

    /home 10.0.0.64/27(rw,sync,no_root_squash,no_all_squash)

* **10.0.0.64/27:** Restricts access to IPs 10.0.0.65 through 10.0.0.94.
* **rw:** Read and Write access.
* **sync:** Confirm writes to disk before replying (data safety).
* **no_root_squash:** Allows the root user on compute nodes to write files as root (useful for administration/logging).

### 2.3 Apply Configuration

Export the file system and configure the firewall to allow NFS traffic.

    exportfs -r
    firewall-cmd --permanent --add-service=nfs
    firewall-cmd --permanent --add-service=mountd
    firewall-cmd --permanent --add-service=rpc-bind
    firewall-cmd --reload

## 3. Create the Software Repository

We need a dedicated location to store the scientific software containers (Apptainer images). We will place this inside `/home` so it is automatically shared.

Create the directory and assign ownership to the admin user `eal`.

    mkdir -p /home/software/containers
    chown -R eal:eal /home/software
    chmod -R 755 /home/software

* **755 Permissions:** `eal` has full control (Read/Write/Execute). All other users have Read/Execute access (can run containers, but cannot delete them).

## 4. Client-Side Configuration (Compute Nodes)

Now we must tell the compute nodes to mount this share automatically when they boot. We will use a **System Overlay** to inject a custom `/etc/fstab`.

### 4.1 Create the NFS Overlay

Create a new overlay structure named `nfs-client`:

    sudo wwctl overlay create nfs-client

### 4.2 Configure fstab

Edit the filesystem table in the overlay.

    sudo wwctl overlay edit nfs-client /etc/fstab

Add the following line to the end of the file. This instructs the node to mount the Head Node's `/home` directory.

    10.0.0.91:/home    /home    nfs    defaults    0 0

### 4.3 Build and Assign

Build the overlay and add it to the `default` profile.

    sudo wwctl overlay build nfs-client
    sudo wwctl profile set default --systemoverlay nfs-client

* **Note:** If you already assigned the `lacp-bond` overlay in Step 2.3, this command *appends* the new overlay. Warewulf profiles can have multiple system overlays (e.g., `lacp-bond,nfs-client`).

## 5. Verification

To verify the setup, we must reboot a compute node and check the mounts.

1. **Reboot Node 02:**

        sudo wwctl power cycle node02

2. **SSH into Node 02:**

        ssh node02

3. **Check Mounts:**

        df -h /home

    **Expected Output:**
    The filesystem should list `10.0.0.91:/home` mounted on `/home` with size matching the Head Node's disk.

4. **Test Write Access:**

        touch /home/testfile_from_node02
        ls -l /home/testfile_from_node02

    If the file appears and no errors occur, the shared storage is active.

## 6. Next Steps

With the shared storage active, the infrastructure is ready to host the scientific software stack.

Proceed to **Phase 3: Scientific Stack & Validation**.
